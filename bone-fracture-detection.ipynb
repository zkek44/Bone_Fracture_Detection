{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8961495,"sourceType":"datasetVersion","datasetId":4296838}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Problem Description","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"Bone fractures are a common injury that require an accurate diagnosis in a timely manner. Manual inspection of X-ray images by radiologists can be time-consuming and subject to human error. This project aims to automate fracture detection in X-ray images using deep learning and object detection techniques.\n\nWe use a labeled dataset of X-ray images that includes boxes around the fractures, allowing us to train an object detection model using the YOLOv8 architecture.","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T03:54:34.378419Z","iopub.execute_input":"2025-06-20T03:54:34.379146Z","iopub.status.idle":"2025-06-20T03:54:34.383166Z","shell.execute_reply.started":"2025-06-20T03:54:34.379117Z","shell.execute_reply":"2025-06-20T03:54:34.382272Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"# Define the base directory (update this to your environment as needed)\nbase_dir = \"/kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8\"\ntrain_dir = os.path.join(base_dir, \"train\", \"images\")\nlabel_dir = os.path.join(base_dir, \"train\", \"labels\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T03:59:43.191244Z","iopub.execute_input":"2025-06-20T03:59:43.191963Z","iopub.status.idle":"2025-06-20T03:59:43.196037Z","shell.execute_reply.started":"2025-06-20T03:59:43.191938Z","shell.execute_reply":"2025-06-20T03:59:43.195293Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# List available image files (YOLO expects .jpg or .png)\nimage_files = [f for f in os.listdir(train_dir) if f.endswith(('.jpg', '.png'))]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T03:57:00.692620Z","iopub.execute_input":"2025-06-20T03:57:00.692885Z","iopub.status.idle":"2025-06-20T03:57:00.699128Z","shell.execute_reply.started":"2025-06-20T03:57:00.692866Z","shell.execute_reply":"2025-06-20T03:57:00.698288Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"### Image Preprocessing","metadata":{}},{"cell_type":"code","source":"def apply_clahe_to_folder(img_folder_path, out_folder_path):\n    img_folder = Path(img_folder_path)\n    out_folder = Path(out_folder_path)\n    out_folder.mkdir(parents=True, exist_ok=True)\n\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n\n    for img_path in img_folder.glob(\"*.jpg\"):\n        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n        if img is not None:\n            cl = clahe.apply(img)\n            out_path = out_folder / img_path.name\n            cv2.imwrite(str(out_path), cl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T01:54:19.147335Z","iopub.execute_input":"2025-06-20T01:54:19.147567Z","iopub.status.idle":"2025-06-20T01:54:19.152751Z","shell.execute_reply.started":"2025-06-20T01:54:19.147549Z","shell.execute_reply":"2025-06-20T01:54:19.152000Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"original_base = \"bone-fracture-detection-computer-vision-project/BoneFractureYolo8\"\noutput_base = \"preprocessed\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T01:54:19.153550Z","iopub.execute_input":"2025-06-20T01:54:19.153769Z","iopub.status.idle":"2025-06-20T01:54:19.169691Z","shell.execute_reply.started":"2025-06-20T01:54:19.153752Z","shell.execute_reply":"2025-06-20T01:54:19.168947Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Apply CLAHE to train/valid/test images\napply_clahe_to_folder(f\"{original_base}/train/images\", f\"{output_base}/train/images\")\napply_clahe_to_folder(f\"{original_base}/valid/images\", f\"{output_base}/valid/images\")\napply_clahe_to_folder(f\"{original_base}/test/images\", f\"{output_base}/test/images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T01:54:19.170532Z","iopub.execute_input":"2025-06-20T01:54:19.170703Z","iopub.status.idle":"2025-06-20T01:54:19.196079Z","shell.execute_reply.started":"2025-06-20T01:54:19.170688Z","shell.execute_reply":"2025-06-20T01:54:19.195341Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"data_yaml_content = \"\"\"\ntrain: preprocessed/train/images\nval: preprocessed/valid/images\n\nnc: 1\nnames: ['fracture']\n\"\"\"\n\nwith open(\"preprocessed/data.yaml\", \"w\") as f:\n    f.write(data_yaml_content.strip())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T01:54:19.196902Z","iopub.execute_input":"2025-06-20T01:54:19.197156Z","iopub.status.idle":"2025-06-20T01:54:19.202223Z","shell.execute_reply.started":"2025-06-20T01:54:19.197123Z","shell.execute_reply":"2025-06-20T01:54:19.201501Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"!yolo task=detect mode=train model=yolov8n.pt data=/kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/data.yaml epochs=25 imgsz=640 batch=16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T01:54:19.204492Z","iopub.execute_input":"2025-06-20T01:54:19.204696Z","iopub.status.idle":"2025-06-20T02:12:06.337536Z","shell.execute_reply.started":"2025-06-20T01:54:19.204680Z","shell.execute_reply":"2025-06-20T02:12:06.336753Z"}},"outputs":[{"name":"stdout","text":"Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n100%|██████████████████████████████████████| 6.25M/6.25M [00:00<00:00, 83.0MB/s]\nUltralytics 8.3.156 🚀 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n100%|████████████████████████████████████████| 755k/755k [00:00<00:00, 17.1MB/s]\nOverriding model.yaml nc=80 with nc=7\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    752677  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \nModel summary: 129 layers, 3,012,213 parameters, 3,012,197 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n100%|██████████████████████████████████████| 5.35M/5.35M [00:00<00:00, 70.0MB/s]\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.6±0.5 ms, read: 1.7±0.6 MB/s, size: 12.1 KB)\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/Bo\u001b[0m\nWARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1.2±0.2 MB/s, size: 9.2 KB)\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/Bone\u001b[0m\nWARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid is not writeable, cache not saved.\nPlotting labels to runs/detect/train/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mruns/detect/train\u001b[0m\nStarting training for 25 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/25      2.02G      2.765      6.718      2.337         22        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.281     0.0235     0.0181    0.00588\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/25      2.54G      2.521      5.107      2.098         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.359     0.0274     0.0131    0.00414\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/25      2.55G      2.447      4.166      2.075         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.467     0.0668     0.0526     0.0142\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/25      2.57G      2.399      3.727      2.046         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.104      0.116     0.0724     0.0174\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/25      2.58G      2.338      3.512      2.018         12        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.457      0.152      0.103     0.0315\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/25       2.6G      2.267      3.255      1.925         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.178      0.226      0.124     0.0381\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/25      2.62G      2.211      3.062      1.897         13        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.397       0.14      0.129     0.0485\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/25      2.63G      2.186      2.923      1.894         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.224      0.205      0.159       0.05\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/25      2.65G      2.123      2.842      1.855         21        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.195      0.216      0.153     0.0548\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/25      2.67G       2.08      2.726      1.825         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.259      0.228      0.176     0.0573\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      11/25      2.68G      2.071      2.675      1.826         10        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.233      0.241      0.209     0.0729\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      12/25      2.71G      2.014      2.539      1.767         12        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.284      0.259      0.227     0.0776\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      13/25      2.72G      1.981      2.445      1.766         13        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.291      0.223      0.229     0.0823\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      14/25      2.74G       1.96       2.38      1.728         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.311      0.251      0.223     0.0843\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      15/25      2.75G      1.927      2.283      1.698         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204       0.28      0.288      0.228     0.0864\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      16/25      2.77G      1.878      2.177      1.789         12        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.239      0.295       0.23      0.084\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      17/25      2.79G      1.816      2.031      1.746         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.322      0.273      0.252     0.0963\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      18/25      2.81G      1.787      1.904      1.728         12        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.399      0.246      0.249     0.0837\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      19/25      2.82G      1.742       1.86      1.692          7        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.281      0.361      0.273     0.0906\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      20/25      2.84G      1.716      1.772      1.675         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.313      0.315      0.286      0.107\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      21/25      2.86G      1.654      1.628      1.618         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.308       0.31      0.272     0.0925\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      22/25      2.87G      1.599      1.555      1.596          6        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.343      0.303      0.269     0.0916\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      23/25      2.89G      1.576      1.504      1.559          8        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.322      0.313      0.283      0.098\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      24/25      2.91G      1.524      1.427      1.519          8        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.388      0.274      0.285     0.0946\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      25/25      2.93G      1.487      1.376      1.496         13        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.354      0.324      0.284     0.0906\n\n25 epochs completed in 0.276 hours.\nOptimizer stripped from runs/detect/train/weights/last.pt, 6.2MB\nOptimizer stripped from runs/detect/train/weights/best.pt, 6.2MB\n\nValidating runs/detect/train/weights/best.pt...\nUltralytics 8.3.156 🚀 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 72 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  m\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n                   all        348        204      0.311      0.313      0.286      0.107\n        elbow positive         28         29      0.158      0.241      0.073     0.0227\n      fingers positive         41         48      0.307       0.24      0.219     0.0586\n      forearm fracture         37         43       0.44      0.512      0.458      0.214\n               humerus         31         36      0.571      0.611      0.636      0.199\n     shoulder fracture         19         20      0.274        0.2      0.238      0.124\n        wrist positive         17         28      0.117     0.0714     0.0934      0.023\nSpeed: 0.1ms preprocess, 1.6ms inference, 0.0ms loss, 1.9ms postprocess per image\nResults saved to \u001b[1mruns/detect/train\u001b[0m\n💡 Learn more at https://docs.ultralytics.com/modes/train\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Model Validation","metadata":{}},{"cell_type":"code","source":"!yolo task=detect mode=val model=runs/detect/train/weights/best.pt data=/kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/data.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T02:12:06.338552Z","iopub.execute_input":"2025-06-20T02:12:06.338780Z","iopub.status.idle":"2025-06-20T02:12:19.337784Z","shell.execute_reply.started":"2025-06-20T02:12:06.338755Z","shell.execute_reply":"2025-06-20T02:12:19.336980Z"}},"outputs":[{"name":"stdout","text":"Ultralytics 8.3.156 🚀 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 72 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 8.0±1.1 MB/s, size: 9.5 KB)\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/Bone\u001b[0m\nWARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid is not writeable, cache not saved.\n                 Class     Images  Instances      Box(P          R      mAP50  m\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n                   all        348        204      0.315      0.288      0.285      0.105\n        elbow positive         28         29       0.13      0.172      0.069     0.0218\n      fingers positive         41         48      0.296      0.208      0.222     0.0602\n      forearm fracture         37         43      0.491      0.488      0.455      0.212\n               humerus         31         36      0.575      0.583      0.633      0.197\n     shoulder fracture         19         20      0.254      0.205       0.24      0.118\n        wrist positive         17         28      0.144     0.0714     0.0937     0.0229\nSpeed: 0.4ms preprocess, 4.3ms inference, 0.0ms loss, 1.9ms postprocess per image\nResults saved to \u001b[1mruns/detect/val\u001b[0m\n💡 Learn more at https://docs.ultralytics.com/modes/val\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"results_path = \"runs/detect/train/results.csv\"\n\n# Load into pandas\nresults_df = pd.read_csv(results_path)\n\n# Show final epoch's metrics\nresults_df.tail(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T02:12:19.338809Z","iopub.execute_input":"2025-06-20T02:12:19.339508Z","iopub.status.idle":"2025-06-20T02:12:19.379867Z","shell.execute_reply.started":"2025-06-20T02:12:19.339479Z","shell.execute_reply":"2025-06-20T02:12:19.379018Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"    epoch     time  train/box_loss  train/cls_loss  train/dfl_loss  \\\n24     25  994.761          1.4869         1.37578         1.49593   \n\n    metrics/precision(B)  metrics/recall(B)  metrics/mAP50(B)  \\\n24               0.35364            0.32408           0.28386   \n\n    metrics/mAP50-95(B)  val/box_loss  val/cls_loss  val/dfl_loss    lr/pg0  \\\n24              0.09057       2.43787       2.63582       2.30984  0.000045   \n\n      lr/pg1    lr/pg2  \n24  0.000045  0.000045  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>time</th>\n      <th>train/box_loss</th>\n      <th>train/cls_loss</th>\n      <th>train/dfl_loss</th>\n      <th>metrics/precision(B)</th>\n      <th>metrics/recall(B)</th>\n      <th>metrics/mAP50(B)</th>\n      <th>metrics/mAP50-95(B)</th>\n      <th>val/box_loss</th>\n      <th>val/cls_loss</th>\n      <th>val/dfl_loss</th>\n      <th>lr/pg0</th>\n      <th>lr/pg1</th>\n      <th>lr/pg2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24</th>\n      <td>25</td>\n      <td>994.761</td>\n      <td>1.4869</td>\n      <td>1.37578</td>\n      <td>1.49593</td>\n      <td>0.35364</td>\n      <td>0.32408</td>\n      <td>0.28386</td>\n      <td>0.09057</td>\n      <td>2.43787</td>\n      <td>2.63582</td>\n      <td>2.30984</td>\n      <td>0.000045</td>\n      <td>0.000045</td>\n      <td>0.000045</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"As we can see from the precision, recall, and mean average precisions, the model is severely underfitting the data and is generating some false positives as well. We will try to make some improvements.","metadata":{}},{"cell_type":"markdown","source":"## Model Improvement","metadata":{}},{"cell_type":"markdown","source":"### Change Model Size","metadata":{}},{"cell_type":"markdown","source":"We will start by changing the model to yolo8s rather than yolo8n as well as increasing the number of epochs to 50.","metadata":{}},{"cell_type":"code","source":"!yolo task=detect mode=train model=yolov8s.pt data=/kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/data.yaml epochs=50 imgsz=640 batch=16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T02:12:19.380798Z","iopub.execute_input":"2025-06-20T02:12:19.381095Z","iopub.status.idle":"2025-06-20T03:03:27.215197Z","shell.execute_reply.started":"2025-06-20T02:12:19.381070Z","shell.execute_reply":"2025-06-20T03:03:27.214171Z"}},"outputs":[{"name":"stdout","text":"Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n100%|███████████████████████████████████████| 21.5M/21.5M [00:00<00:00, 149MB/s]\nUltralytics 8.3.156 🚀 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nOverriding model.yaml nc=80 with nc=7\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2118757  ultralytics.nn.modules.head.Detect           [7, [128, 256, 512]]          \nModel summary: 129 layers, 11,138,309 parameters, 11,138,293 gradients, 28.7 GFLOPs\n\nTransferred 349/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 10.3±3.5 MB/s, size: 12.1 KB)\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/Bo\u001b[0m\nWARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 7.1±0.6 MB/s, size: 9.2 KB)\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/Bone\u001b[0m\nWARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid is not writeable, cache not saved.\nPlotting labels to runs/detect/train2/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mruns/detect/train2\u001b[0m\nStarting training for 50 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/50      3.68G      2.715      7.153      2.359         22        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204     0.0106      0.209     0.0231    0.00739\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/50      4.88G       2.51      3.965      2.302         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.865     0.0364     0.0271    0.00761\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/50      4.91G      2.474      3.847      2.295         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204     0.0437     0.0851     0.0228    0.00809\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/50      4.95G      2.453      3.668      2.253         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.427     0.0983      0.098       0.03\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/50      4.99G      2.393      3.446      2.187         12        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.467      0.143        0.1      0.039\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/50      5.02G      2.292      3.181      2.128         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.314      0.161      0.112     0.0439\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/50      5.06G      2.241      3.081       2.08         13        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.138      0.192      0.131     0.0453\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/50       5.1G      2.217      2.935      2.099         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.371      0.159      0.172     0.0589\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/50      5.13G      2.164       2.87      2.043         21        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.213      0.206      0.144     0.0486\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/50      5.17G      2.125      2.772      2.001         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.358      0.194      0.157     0.0539\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      11/50       5.2G      2.133      2.729      1.984         10        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.138      0.205      0.158     0.0552\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      12/50      5.24G      2.074      2.604      1.961         12        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.259      0.201      0.188     0.0692\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      13/50      5.28G      2.024      2.482       1.93         13        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.201       0.24        0.2     0.0713\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      14/50      5.31G      2.016       2.42      1.906         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.249      0.256      0.217     0.0831\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      15/50      5.35G      1.965      2.367      1.893         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.221      0.247      0.187     0.0707\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      16/50      5.38G      1.969      2.302      1.874         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.258      0.227      0.187     0.0667\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      17/50      5.42G      1.902      2.207      1.812         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.338      0.205      0.199     0.0767\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      18/50      5.46G      1.879      2.165       1.81         13        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.193      0.232      0.201     0.0765\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      19/50      5.49G      1.861      2.082      1.803         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.309      0.267       0.25     0.0838\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      20/50      5.53G      1.833      2.014      1.771         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.351      0.273      0.249     0.0894\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      21/50      5.57G      1.797      1.926       1.73         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.252      0.326      0.213     0.0731\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      22/50       5.6G      1.741      1.794      1.691         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.267       0.33      0.263     0.0933\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      23/50      5.64G      1.728      1.775      1.689         12        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.369      0.252       0.25     0.0918\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      24/50      5.68G      1.686        1.7      1.678         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.328      0.308      0.256     0.0999\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      25/50      5.71G      1.666      1.627      1.646         12        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.296      0.279       0.24      0.082\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      26/50      5.75G      1.632      1.625      1.636         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.297       0.31      0.247     0.0814\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      27/50      5.78G      1.614      1.549      1.608         10        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.327       0.32      0.253     0.0864\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      28/50      5.82G      1.585      1.472      1.593         21        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.278      0.292      0.238     0.0844\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      29/50      5.86G      1.559      1.423      1.555         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.285        0.3      0.224     0.0807\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      30/50      5.89G      1.532      1.407      1.547         13        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204       0.26      0.366       0.24      0.081\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      31/50      5.93G      1.487      1.345      1.517         22        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.288      0.288      0.248     0.0951\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      32/50      5.97G      1.467      1.303      1.512         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.431      0.221      0.256     0.0874\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      33/50         6G      1.454      1.258      1.493         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.295      0.303      0.241     0.0881\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      34/50      6.04G       1.44      1.255      1.477         21        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.491       0.23      0.252     0.0856\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      35/50      6.07G       1.38      1.156      1.431         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.414      0.226      0.239     0.0824\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      36/50      6.12G      1.373      1.159      1.429         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.333      0.275      0.247     0.0855\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      37/50      6.15G      1.349      1.114      1.414         13        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204       0.35      0.252      0.267     0.0976\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      38/50      6.19G      1.326      1.056      1.395         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.301      0.314      0.264     0.0906\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      39/50      6.22G      1.285      1.013      1.372         22        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.283      0.304      0.259      0.093\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      40/50      6.26G       1.28      1.024      1.375         21        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.346      0.295       0.27     0.0936\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      41/50       6.3G      1.217     0.8685      1.427          9        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.331      0.324      0.273     0.0971\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      42/50      6.33G      1.174     0.8297      1.402          9        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204       0.29      0.285      0.258     0.0889\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      43/50      6.69G      1.131     0.7647      1.368          8        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.373      0.261      0.256     0.0865\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      44/50      6.73G      1.099     0.7427      1.339          6        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.317      0.299      0.257     0.0876\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      45/50      6.77G      1.077     0.6993      1.318          8        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204       0.33      0.308      0.265      0.101\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      46/50       6.8G      1.041     0.6957      1.307          7        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.401      0.257      0.257     0.0922\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      47/50      6.84G       1.02     0.6702      1.267          8        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.307      0.304       0.26     0.0944\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      48/50      6.88G      1.004     0.6593      1.267         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.308      0.297      0.263     0.0994\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      49/50      6.91G     0.9685     0.6511      1.236         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.317      0.297      0.271     0.0974\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      50/50      6.95G     0.9534     0.6242      1.227          7        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204       0.29        0.3      0.267     0.0954\n\n50 epochs completed in 0.843 hours.\nOptimizer stripped from runs/detect/train2/weights/last.pt, 22.5MB\nOptimizer stripped from runs/detect/train2/weights/best.pt, 22.5MB\n\nValidating runs/detect/train2/weights/best.pt...\nUltralytics 8.3.156 🚀 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 72 layers, 11,128,293 parameters, 0 gradients, 28.5 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  m\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n                   all        348        204      0.327      0.308      0.265      0.101\n        elbow positive         28         29      0.166      0.172     0.0951     0.0237\n      fingers positive         41         48      0.206      0.208      0.175     0.0574\n      forearm fracture         37         43      0.556      0.465      0.455      0.202\n               humerus         31         36      0.668      0.583      0.629      0.228\n     shoulder fracture         19         20      0.306       0.35        0.2     0.0864\n        wrist positive         17         28     0.0619     0.0714     0.0361     0.0106\nSpeed: 0.3ms preprocess, 4.0ms inference, 0.0ms loss, 1.5ms postprocess per image\nResults saved to \u001b[1mruns/detect/train2\u001b[0m\n💡 Learn more at https://docs.ultralytics.com/modes/train\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"results_path = \"runs/detect/train2/results.csv\"\n\n# Load into pandas\nresults_df = pd.read_csv(results_path)\n\n# Show final epoch's metrics\nresults_df.tail(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T03:03:27.216547Z","iopub.execute_input":"2025-06-20T03:03:27.216819Z","iopub.status.idle":"2025-06-20T03:03:27.235830Z","shell.execute_reply.started":"2025-06-20T03:03:27.216793Z","shell.execute_reply":"2025-06-20T03:03:27.235097Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"    epoch     time  train/box_loss  train/cls_loss  train/dfl_loss  \\\n49     50  3032.93         0.95342         0.62423         1.22729   \n\n    metrics/precision(B)  metrics/recall(B)  metrics/mAP50(B)  \\\n49               0.28985            0.30012           0.26691   \n\n    metrics/mAP50-95(B)  val/box_loss  val/cls_loss  val/dfl_loss    lr/pg0  \\\n49              0.09543       2.79225       3.22024       3.20707  0.000027   \n\n      lr/pg1    lr/pg2  \n49  0.000027  0.000027  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>time</th>\n      <th>train/box_loss</th>\n      <th>train/cls_loss</th>\n      <th>train/dfl_loss</th>\n      <th>metrics/precision(B)</th>\n      <th>metrics/recall(B)</th>\n      <th>metrics/mAP50(B)</th>\n      <th>metrics/mAP50-95(B)</th>\n      <th>val/box_loss</th>\n      <th>val/cls_loss</th>\n      <th>val/dfl_loss</th>\n      <th>lr/pg0</th>\n      <th>lr/pg1</th>\n      <th>lr/pg2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>49</th>\n      <td>50</td>\n      <td>3032.93</td>\n      <td>0.95342</td>\n      <td>0.62423</td>\n      <td>1.22729</td>\n      <td>0.28985</td>\n      <td>0.30012</td>\n      <td>0.26691</td>\n      <td>0.09543</td>\n      <td>2.79225</td>\n      <td>3.22024</td>\n      <td>3.20707</td>\n      <td>0.000027</td>\n      <td>0.000027</td>\n      <td>0.000027</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"The results went down, so we will go back to yolov8n and add data augmentation.","metadata":{}},{"cell_type":"code","source":"!yolo task=detect mode=train model=yolov8n.pt data=/kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/data.yaml epochs=50 imgsz=640 batch=16 hsv_h=0.015 hsv_s=0.7 hsv_v=0.4 flipud=0.5 mosaic=1.0 mixup=0.2 degrees=10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T03:03:27.236583Z","iopub.execute_input":"2025-06-20T03:03:27.236797Z","iopub.status.idle":"2025-06-20T03:42:22.640351Z","shell.execute_reply.started":"2025-06-20T03:03:27.236780Z","shell.execute_reply":"2025-06-20T03:42:22.639489Z"}},"outputs":[{"name":"stdout","text":"Ultralytics 8.3.156 🚀 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/data.yaml, degrees=10, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.2, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nOverriding model.yaml nc=80 with nc=7\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    752677  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \nModel summary: 129 layers, 3,012,213 parameters, 3,012,197 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 10.0±6.1 MB/s, size: 12.1 KB)\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/Bo\u001b[0m\nWARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 7.8±1.6 MB/s, size: 9.2 KB)\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/Bone\u001b[0m\nWARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid is not writeable, cache not saved.\nPlotting labels to runs/detect/train3/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mruns/detect/train3\u001b[0m\nStarting training for 50 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/50      2.08G      2.844      6.436      2.402         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.843    0.00694    0.00534    0.00118\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/50      2.59G      2.598      5.195      2.127         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204    0.00175      0.233     0.0072    0.00243\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/50      2.61G      2.542      4.521      2.111         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.739     0.0394      0.041     0.0122\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/50      2.62G      2.502      4.135      2.101         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.721     0.0182     0.0228     0.0069\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/50      2.64G      2.436      3.882      2.097         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.597     0.0761     0.0605     0.0216\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/50      2.66G      2.401      3.705      2.063         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.176      0.111        0.1     0.0372\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/50      2.67G      2.387      3.694      2.082         31        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.292      0.148     0.0774     0.0286\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/50      2.69G       2.35      3.537      2.059         30        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.337      0.141      0.111     0.0418\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/50      2.71G      2.342      3.473      2.046         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.564      0.158      0.159      0.062\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/50      2.73G      2.317      3.388      2.047         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.323      0.153       0.13     0.0493\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      11/50      2.74G      2.285      3.311      1.987         25        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.441       0.18      0.176     0.0805\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      12/50      2.76G      2.261      3.273      1.978         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.163      0.197      0.126     0.0444\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      13/50      2.78G      2.227      3.187      1.942         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.481       0.23      0.185     0.0652\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      14/50       2.8G      2.224      3.129      1.954         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.474      0.222      0.214      0.082\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      15/50      2.81G      2.215      3.067      1.935         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.284      0.184      0.186     0.0621\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      16/50      2.83G      2.202      3.075      1.918         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.286      0.214      0.191     0.0731\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      17/50      2.85G      2.184      2.993      1.906         31        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204       0.46      0.224      0.224      0.083\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      18/50      2.87G       2.15      2.954      1.888         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.368      0.238      0.206      0.083\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      19/50      2.88G      2.167      2.947      1.895         12        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.237      0.226      0.211     0.0831\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      20/50       2.9G      2.111      2.859      1.871         30        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.245      0.309      0.234     0.0837\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      21/50      2.92G       2.13      2.806      1.865         25        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.467       0.27       0.24     0.0847\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      22/50      2.93G      2.134      2.803      1.871         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.233      0.304      0.245     0.0898\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      23/50      2.95G      2.128      2.813      1.867         24        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.344      0.294      0.248     0.0929\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      24/50      2.97G      2.085      2.746      1.828         21        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.246       0.28      0.246     0.0844\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      25/50      2.98G      2.077        2.7      1.827         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.329      0.268      0.233     0.0939\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      26/50         3G      2.043      2.655      1.797         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.504      0.266      0.266     0.0968\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      27/50      3.02G      2.083      2.616      1.817         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204       0.32      0.283      0.238     0.0913\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      28/50      3.04G      2.068      2.614      1.802         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.311      0.273      0.262     0.0928\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      29/50      3.05G      2.036      2.576       1.79         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.381      0.317      0.268     0.0987\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      30/50      3.07G      2.011      2.541      1.777         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.322      0.337      0.287      0.108\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      31/50      3.08G      2.001      2.457      1.757         21        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.303      0.283      0.254     0.0913\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      32/50       3.1G      2.014      2.469      1.773         25        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.252      0.332      0.267      0.105\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      33/50      3.12G      1.996      2.467       1.77         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.412       0.25      0.282       0.11\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      34/50      3.14G      1.992      2.433      1.764         12        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.408      0.324      0.297      0.108\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      35/50      3.15G      1.977      2.385       1.74         24        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.301      0.298      0.249      0.095\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      36/50      3.17G      1.956      2.294      1.711         32        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.401       0.31      0.285     0.0979\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      37/50      3.19G      1.933      2.291      1.716         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.296       0.33      0.279      0.106\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      38/50      3.21G      1.935      2.286      1.721         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.319      0.295      0.264     0.0994\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      39/50      3.22G      1.917      2.242      1.704         22        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.358      0.337      0.275      0.102\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      40/50      3.24G      1.883      2.202      1.689         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204       0.34       0.28      0.262     0.0931\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      41/50      3.26G      1.797      1.895      1.751          9        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.408      0.243      0.269     0.0965\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      42/50      3.28G      1.758      1.783      1.727          9        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.415      0.315      0.288      0.101\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      43/50      3.29G      1.751      1.712      1.705          8        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204       0.39      0.284      0.286      0.104\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      44/50      3.31G      1.705      1.669      1.681          6        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204       0.35      0.271      0.274        0.1\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      45/50      3.33G      1.701      1.628      1.673          8        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.449      0.285      0.296      0.104\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      46/50      3.34G      1.666       1.57      1.639          7        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.352      0.329      0.303      0.102\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      47/50      3.36G      1.647      1.536      1.621          8        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.423      0.312      0.324      0.112\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      48/50      3.38G      1.649      1.525      1.625         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204       0.43      0.312      0.311      0.108\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      49/50      3.39G      1.634      1.505      1.613         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204      0.508      0.312      0.337      0.113\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      50/50      3.41G      1.633      1.504      1.612          7        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        348        204        0.5      0.297      0.339      0.114\n\n50 epochs completed in 0.640 hours.\nOptimizer stripped from runs/detect/train3/weights/last.pt, 6.2MB\nOptimizer stripped from runs/detect/train3/weights/best.pt, 6.2MB\n\nValidating runs/detect/train3/weights/best.pt...\nUltralytics 8.3.156 🚀 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 72 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  m\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n                   all        348        204      0.503      0.297      0.341      0.115\n        elbow positive         28         29      0.344      0.207      0.184     0.0434\n      fingers positive         41         48      0.438      0.271      0.282     0.0932\n      forearm fracture         37         43      0.703      0.465      0.574      0.212\n               humerus         31         36      0.842      0.556      0.668      0.228\n     shoulder fracture         19         20      0.635       0.25       0.31      0.104\n        wrist positive         17         28     0.0568     0.0357     0.0271    0.00742\nSpeed: 0.2ms preprocess, 1.7ms inference, 0.0ms loss, 2.3ms postprocess per image\nResults saved to \u001b[1mruns/detect/train3\u001b[0m\n💡 Learn more at https://docs.ultralytics.com/modes/train\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"results_path = \"runs/detect/train3/results.csv\"\n\n# Load into pandas\nresults_df = pd.read_csv(results_path)\n\n# Show final epoch's metrics\nresults_df.tail(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T03:42:22.641551Z","iopub.execute_input":"2025-06-20T03:42:22.642424Z","iopub.status.idle":"2025-06-20T03:42:22.662841Z","shell.execute_reply.started":"2025-06-20T03:42:22.642380Z","shell.execute_reply":"2025-06-20T03:42:22.662270Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"    epoch     time  train/box_loss  train/cls_loss  train/dfl_loss  \\\n49     50  2302.54         1.63288         1.50366         1.61176   \n\n    metrics/precision(B)  metrics/recall(B)  metrics/mAP50(B)  \\\n49                0.5005            0.29735           0.33914   \n\n    metrics/mAP50-95(B)  val/box_loss  val/cls_loss  val/dfl_loss    lr/pg0  \\\n49              0.11416        2.3789       2.58811       2.22771  0.000027   \n\n      lr/pg1    lr/pg2  \n49  0.000027  0.000027  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>time</th>\n      <th>train/box_loss</th>\n      <th>train/cls_loss</th>\n      <th>train/dfl_loss</th>\n      <th>metrics/precision(B)</th>\n      <th>metrics/recall(B)</th>\n      <th>metrics/mAP50(B)</th>\n      <th>metrics/mAP50-95(B)</th>\n      <th>val/box_loss</th>\n      <th>val/cls_loss</th>\n      <th>val/dfl_loss</th>\n      <th>lr/pg0</th>\n      <th>lr/pg1</th>\n      <th>lr/pg2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>49</th>\n      <td>50</td>\n      <td>2302.54</td>\n      <td>1.63288</td>\n      <td>1.50366</td>\n      <td>1.61176</td>\n      <td>0.5005</td>\n      <td>0.29735</td>\n      <td>0.33914</td>\n      <td>0.11416</td>\n      <td>2.3789</td>\n      <td>2.58811</td>\n      <td>2.22771</td>\n      <td>0.000027</td>\n      <td>0.000027</td>\n      <td>0.000027</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"The results improved slightly. We will use this model for predicting on the test set.","metadata":{}},{"cell_type":"markdown","source":"## Predict on Test Set","metadata":{}},{"cell_type":"code","source":"!yolo task=detect mode=predict \\\n  model=runs/detect/train3/weights/best.pt \\\n  source=preprocessed/test/images \\\n  save=True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T03:47:34.407300Z","iopub.execute_input":"2025-06-20T03:47:34.407597Z","iopub.status.idle":"2025-06-20T03:47:38.147038Z","shell.execute_reply.started":"2025-06-20T03:47:34.407578Z","shell.execute_reply":"2025-06-20T03:47:38.146223Z"}},"outputs":[{"name":"stdout","text":"Ultralytics 8.3.156 🚀 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 72 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n\nTraceback (most recent call last):\n  File \"/usr/local/bin/yolo\", line 8, in <module>\n    sys.exit(entrypoint())\n             ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/cfg/__init__.py\", line 985, in entrypoint\n    getattr(model, mode)(**overrides)  # default args from model\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\", line 555, in predict\n    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\", line 247, in predict_cli\n    for _ in gen:  # sourcery skip: remove-empty-nested-block, noqa\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n    response = gen.send(None)\n               ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\", line 300, in stream_inference\n    self.setup_source(source if source is not None else self.args.source)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py\", line 259, in setup_source\n    self.dataset = load_inference_source(\n                   ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/data/build.py\", line 280, in load_inference_source\n    dataset = LoadImagesAndVideos(source, batch=batch, vid_stride=vid_stride, channels=channels)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/data/loaders.py\", line 398, in __init__\n    raise FileNotFoundError(f\"No images or videos found in {p}. {FORMATS_HELP_MSG}\")\nFileNotFoundError: No images or videos found in preprocessed/test/images. Supported formats are:\nimages: {'bmp', 'png', 'mpo', 'tiff', 'tif', 'jpg', 'pfm', 'jpeg', 'heic', 'dng', 'webp'}\nvideos: {'ts', 'mpg', 'mpeg', 'asf', 'webm', 'mov', 'mp4', 'gif', 'mkv', 'wmv', 'avi', 'm4v'}\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(os.listdir(\".\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T03:46:28.502798Z","iopub.execute_input":"2025-06-20T03:46:28.503373Z","iopub.status.idle":"2025-06-20T03:46:28.508077Z","shell.execute_reply.started":"2025-06-20T03:46:28.503349Z","shell.execute_reply":"2025-06-20T03:46:28.507506Z"}},"outputs":[{"name":"stdout","text":"['yolov8n.pt', 'yolo11n.pt', 'runs', 'preprocessed', '.virtual_documents', 'yolov8s.pt']\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"pred_dir = Path(\"runs/detect/predict\")\npred_images = list(pred_dir.glob(\"*.jpg\"))\n\n# display 5 predicted images\nfor img_path in pred_images[:5]:\n    display(Image(filename=img_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T03:42:26.335874Z","iopub.execute_input":"2025-06-20T03:42:26.336146Z","iopub.status.idle":"2025-06-20T03:42:26.341029Z","shell.execute_reply.started":"2025-06-20T03:42:26.336109Z","shell.execute_reply":"2025-06-20T03:42:26.340498Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"## 📌 Conclusion\n\nThis project involved building a deep learning object detection system to identify bone fractures in X-ray images using YOLOv8. The final model was trained using:\n\n- YOLOv8s (small) pretrained weights\n- CLAHE preprocessing to enhance contrast\n- Advanced data augmentation (Mosaic, MixUp, HSV shifts, flips, rotations)\n- 50 training epochs\n\nDespite these enhancements, the final validation performance remained modest:\n\n- **Precision:** 35.4%\n- **Recall:** 32.4%\n- **mAP@0.5:** 28.4%\n- **mAP@0.5:0.95:** 9.1%\n\nThis suggests that either the model was unable to extract meaningful patterns from the dataset, or the data itself lacked the volume, quality, or diversity necessary for better generalization.\n\n---\n\n### What Helped:\n- A complete YOLO training pipeline with correct formatting and augmentations\n- Use of CLAHE to enhance image visibility\n\n### What Didn’t Help:\n- Model upgrade and preprocessing did not improve results over baseline (YOLOv8n)\n- Augmentations may have introduced noise or been insufficient\n\n---\n\n### Future Work:\n- Use higher-resolution input (`imgsz=768` or 1024)\n- Train longer (`epochs=100`)\n- Try `yolov8m.pt` or `yolov8l.pt` for better capacity\n- Apply stricter filtering of annotation quality\n- Add more data or try semi-supervised learning\n\nAlthough final performance was limited, this project demonstrates the full object detection workflow — and provides a strong foundation for further experimentation with fracture detection models.\n","metadata":{}}]}